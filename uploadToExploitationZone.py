from pyspark.sql import SparkSession
from hdfs import InsecureClient

hdfs_server = "hdfs://10.4.41.46:27000"
hdfs_source_path = "/user/bdm/formatted_zone"
hdfs_cli = InsecureClient("http://10.4.41.46:9870", user="bdm")
subdirs = [f"{hdfs_source_path}/{name}" for name in hdfs_cli.list(hdfs_source_path) if hdfs_cli.status(f"{hdfs_source_path}/{name}")['type'] == 'DIRECTORY']
subdir = '/user/bdm/formatted_zone/idealista'
files = [f"{subdir}/{name}" for name in hdfs_cli.list('/user/bdm/formatted_zone/idealista')]
target_hdfs_path = "/user/bdm/exploitation_zone"
csv_target_folder = '/tmp/pycharm_project_743/data/'

def startSpark():
    """
    start spark session
    """
    spark = SparkSession.builder \
        .appName("Read Parquet Files") \
        .getOrCreate()
    spark.conf.set("spark.sql.parser.escapedStringLiterals", "true")
    return spark

def readIdealistaFiles(sprak, files):
    """
    read all idealista files and store it in rdd
    :param sprak:
    :param files:
    :return: rdd
    """
    flag = True
    for file in files:
        if (not(file.endswith('.parquet'))):
            continue
        df = sprak.read.parquet(hdfs_server + file)
        rdd = df.rdd
        if flag:
            fullRDD = rdd.cache()
            flag = False
        else:
            fullRDD = fullRDD.union(rdd).cache()
    return fullRDD

def readParquetFiles(spark, folder_name):
    """
    read parquet files and store it in rdd
    :param spark:
    :param folder_name:
    :return: rdd
    """
    rdd = spark.read.parquet(hdfs_server + hdfs_source_path + '/' + folder_name).rdd
    return rdd

def printRDD(rdd):
    for i in rdd.take(5):
        print(i)

def getAvgListPerNbr(rdd):
    """
    calculated KPI for average price per each neighborhood
    write it to the server storage
    :param rdd:
    """
    output_csv_files = csv_target_folder + 'avg_price/'
    district_neighborhood_prices = rdd.map(lambda row: ((row.neighborhood), (row.price, 1))).cache()
    total_price_count = district_neighborhood_prices\
        .reduceByKey(lambda a, b: (a[0] + b[0], a[1] + b[1]))
    average_price = total_price_count.mapValues(lambda x: round(x[0] / x[1],2)).sortBy(lambda x: x[1],ascending = False)
    column_names = ["Neighborhood", "Average_Price"]
    df = average_price.toDF()
    for i, column_name in enumerate(column_names):
        df = df.withColumnRenamed("_" + str(i + 1), column_name)
    df.coalesce(1).write.option("header","true").mode("overwrite").csv(output_csv_files)

def getROI(idealistaRDD, rentRDD):
    """
    get the rent price per month and divide it by the price of a property in a given neighborhood
    to get the amount of time for ROI
    write the results into a csv in the server storage
    :param idealistaRDD:
    :param rentRDD:
    """
    output_csv_files = csv_target_folder + 'ROI/'
    filteredRDD = rentRDD\
        .filter(lambda row: row.Lloguer_mitja == 'Lloguer mitj√† mensual (Euros/mes)' and row.Preu not in ['NA', 'N/A', '--'])
    avgPriceRDD = filteredRDD.groupBy(lambda row: (row.year, row.neighborhood, row.Lloguer_mitja)) \
        .mapValues(lambda rows: sum(float(row.Preu) for row in rows) / len(rows)) \
        .map(lambda row: ((row[0][0], row[0][1]), row)) \
        .map(lambda row: ((row[0][0], row[0][1]), row[1][1]))
    joinedRDD = idealistaRDD.map(lambda row: ((row.date.year, row.neighborhood), (row.price, row.propertyCode)))\
        .join(avgPriceRDD) \
        .map(lambda row: (row[0][0], row[0][1], row[1][0][1], row[1][0][0] / (12 * row[1][1])))\
        .distinct()
    df = joinedRDD.toDF()
    column_names = ["year", "neighborhood","propertyCode", "ROI/Year"]
    for i, column_name in enumerate(column_names):
        df = df.withColumnRenamed("_" + str(i + 1), column_name)
    df.coalesce(1).write.option("header","true").mode("overwrite").csv(output_csv_files)

def getIdealistaRDD(spark,files):
    return readIdealistaFiles(spark, files)

if __name__ == "__main__":
    spark = startSpark()
    ideaRDD = readIdealistaFiles(spark, files)
    incomeRDD = readParquetFiles(spark, 'income/')
    rentRDD = readParquetFiles(spark, 'rent/')
    getAvgListPerNbr(ideaRDD)
    getROI(ideaRDD,rentRDD)
    print(f"All KPI's were created successfully, files can be seen in path {csv_target_folder}")